{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7107d9f",
   "metadata": {},
   "source": [
    "# Naive Bayes Project\n",
    "\n",
    "Welcome to your Naive Bayes Machine Learning Project! Just follow along with the notebook and instructions below. We will be analyzing the famous sms spam data set from Kaggle!\n",
    "\n",
    "## The Data\n",
    "We will be using the famous [SMS Spam Collection](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset). \n",
    "\n",
    "The SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c8c6c2",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "Let's import some libraries to get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e755bb8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a9c1ff8",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "Let's start by reading in the spam.csv file into a pandas dataframe and perform some operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c60ca6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a89ee545",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf98bf7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae2bf6f1",
   "metadata": {},
   "source": [
    "### Renaming the Features v1 as target and v2 as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6240e8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b0206b8",
   "metadata": {},
   "source": [
    "## Encoding the Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136b84be",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2cf6e7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33a23f12",
   "metadata": {},
   "source": [
    "### Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b167ef",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39f10eaf",
   "metadata": {},
   "source": [
    "### Check for duplicate values and remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04642f73",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69ab0434",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca77ff3",
   "metadata": {},
   "source": [
    "**Using Countplot check the distribution of the target.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8b2ac7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97d02ab8",
   "metadata": {},
   "source": [
    "### Now we have to do some Natural Language Processing on the the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cad58c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d9e7dd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da11976",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aab7b8",
   "metadata": {},
   "source": [
    "It downloads the Punkt model, which is a pre-trained data file that helps the NLTK library split text into lists of words (tokenization) and sentences.\n",
    "\n",
    "Without it, NLTK cannot intelligently break paragraphs into sentences or words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51a8bb4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df['num_characters'] = df['text'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a4f1c1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3617c4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# num of words\n",
    "df['num_words'] = df['text'].apply(lambda x:len(nltk.word_tokenize(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfee60e8",
   "metadata": {},
   "source": [
    "It creates a new column called **`num_words`** that contains the **total count of words** for each message in the `text` column.\n",
    "\n",
    "**Example:**\n",
    "* **Input (`x`):** `\"Hi, how are you?\"`\n",
    "* **Tokenized:** `['Hi', ',', 'how', 'are', 'you', '?']`\n",
    "* **Result (`num_words`):** `6`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a3b777",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df['num_sentences'] = df['text'].apply(lambda x:len(nltk.sent_tokenize(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caddbd8",
   "metadata": {},
   "source": [
    "It creates a new column called **`num_sentences`** that contains the **total count of sentences** for each message.\n",
    "\n",
    "**Example:**\n",
    "* **Input (`x`):** `\"I am fine. How are you?\"`\n",
    "* **Tokenized:** `['I am fine.', 'How are you?']`\n",
    "* **Result (`num_sentences`):** `2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26728e8f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df[['num_characters','num_words','num_sentences']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd803b77",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ham\n",
    "df[df['target'] == 0][['num_characters','num_words','num_sentences']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47cd636",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#spam\n",
    "df[df['target'] == 1][['num_characters','num_words','num_sentences']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b037bd",
   "metadata": {},
   "source": [
    "**Make a histplot for different targets based on number of characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973968dc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1aa4837f",
   "metadata": {},
   "source": [
    "**Make a Pairplot but it should show distinctly the targets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eae0b0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59cd59dc",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "- Lower case\n",
    "- Tokenization\n",
    "- Removing special characters\n",
    "- Removing stop words and punctuation\n",
    "- Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47641ed4",
   "metadata": {},
   "source": [
    "## Text Preprocessing Steps\n",
    "\n",
    "| Step | Definition | Importance (Why?) |\n",
    "| :--- | :--- | :--- |\n",
    "| **Lower Casing** | Converts all text to **lowercase** (e.g., \"The\" $\\rightarrow$ \"the\"). | Ensures the model treats variations like \"Apple\" and \"apple\" as the **same word**, reducing vocabulary size. |\n",
    "| **Tokenization** | Breaks the text into its smallest meaningful units (words, numbers, punctuation). | The **mandatory first step**; it converts a raw string into a list of items for counting and processing. |\n",
    "| **Removing Special Characters** | Eliminates non-alphanumeric symbols (e.g., `@`, `#`, `&`). | Reduces **noise** and keeps the model focused only on characters relevant to language. |\n",
    "| **Removing Stop Words & Punctuation** | Removes common, high-frequency words (e.g., \"a,\" \"the,\" \"is\") and standard punctuation. | Drastically reduces the number of features and forces the model to learn from **meaningful, distinguishing words** (like 'spam' keywords). |\n",
    "| **Stemming** | Reduces a word to its base or root form (e.g., \"running,\" \"runs\" $\\rightarrow$ \"run\"). | Reduces the feature space by treating all grammatical variations of a word as a single feature, helping the model **generalize** better. |\n",
    "\n",
    "***\n",
    "\n",
    "### Overall Importance: Model Effectiveness\n",
    "\n",
    "These steps are vital because they convert raw, messy human language into a clean, consistent, and numerical format that machine learning algorithms can process effectively. They ensure the model is **efficient**, **consistent**, and focuses only on the most **distinguishing information**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3591de72",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def transform_text(text):\n",
    "    text = text.lower()\n",
    "    text = nltk.word_tokenize(text)\n",
    "    \n",
    "    y = []\n",
    "    for i in text:\n",
    "        if i.isalnum():\n",
    "            y.append(i)\n",
    "    \n",
    "    text = y[:]\n",
    "    y.clear()\n",
    "    \n",
    "    for i in text:\n",
    "        if i not in stopwords.words('english') and i not in string.punctuation:\n",
    "            y.append(i)\n",
    "            \n",
    "    text = y[:]\n",
    "    y.clear()\n",
    "    \n",
    "    for i in text:\n",
    "        y.append(ps.stem(i))\n",
    "    \n",
    "            \n",
    "    return \" \".join(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c52c97e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "transform_text(\"I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69c2853",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df['transformed_text'] = df['text'].apply(transform_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d669b27b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3969a795",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "wc = WordCloud(width=500,height=500,min_font_size=10,background_color='white')\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.imshow(spam_wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff416d1e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "ham_wc = wc.generate(df[df['target'] == 0]['transformed_text'].str.cat(sep=\" \"))\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.imshow(ham_wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2235d164",
   "metadata": {},
   "source": [
    "# Model Building\n",
    "\n",
    "##  What is Vectorization?\n",
    "Vectorization is the process of converting text data into numerical data that machine learning models can understand.\n",
    "Vectorization (specifically **Text Vectorization**) is the essential step of turning unstructured text (like a word, a sentence, or a document) into a sequence of numbers, known as a **vector**.\n",
    "\n",
    "For example, a sentence like \"I love cats\" might be converted into the vector: `[0, 1, 0, 1, 2]` where each number corresponds to the presence or frequency of a specific word in the entire vocabulary.\n",
    "\n",
    "### Types:\n",
    "\n",
    "1.  **CountVectorizer (`cv`):** Converts text into a vector by simply counting the frequency of each word in a document.\n",
    "    * *Example:* If the word \"free\" appears 5 times, its corresponding number in the vector is 5.\n",
    "2.  **TfidfVectorizer (`tfidf`):** Converts text into a vector based on **Term Frequency-Inverse Document Frequency (TF-IDF)**. This is a more sophisticated method that weighs word count by how rare or important the word is across *all* documents.\n",
    "    * *Goal:* Give high scores to words that appear often in a *specific* document but rarely in the entire dataset (e.g., \"lottery\" in a spam message).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2df8b92",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "cv = CountVectorizer()\n",
    "tfidf = TfidfVectorizer(max_features=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1ffe0f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "X = tfidf.fit_transform(df['transformed_text']).toarray()\n",
    "y = df['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d99d914",
   "metadata": {},
   "source": [
    "**Train Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3221a5b9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53889af6",
   "metadata": {},
   "source": [
    "# Model Building Prediction and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0c35a4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
