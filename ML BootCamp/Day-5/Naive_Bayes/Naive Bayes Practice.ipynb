{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7107d9f",
   "metadata": {},
   "source": [
    "# Naive Bayes Project\n",
    "\n",
    "Welcome to your Naive Bayes Machine Learning Project! Just follow along with the notebook and instructions below. We will be analyzing the famous sms spam data set from Kaggle!\n",
    "\n",
    "## The Data\n",
    "We will be using the famous [SMS Spam Collection](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset). \n",
    "\n",
    "The SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c8c6c2",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "Let's import some libraries to get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e755bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9c1ff8",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "Let's start by reading in the spam.csv file into a pandas dataframe and perform some operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38c60ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('spam.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b03c069f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "v1",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "v2",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Unnamed: 2",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Unnamed: 3",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Unnamed: 4",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "e1f16525-a89c-4a10-b391-320edb577305",
       "rows": [
        [
         "5020",
         "ham",
         ":-( sad puppy noise",
         null,
         null,
         null
        ],
        [
         "4577",
         "spam",
         "Urgent! call 09066350750 from your landline. Your complimentary 4* Ibiza Holiday or 10,000 cash await collection SAE T&Cs PO BOX 434 SK3 8WP 150 ppm 18+",
         null,
         null,
         null
        ],
        [
         "1360",
         "ham",
         "Yo dude guess who just got arrested the other day",
         null,
         null,
         null
        ],
        [
         "4440",
         "ham",
         "I'm going 2 orchard now laready me reaching soon. U reaching?",
         null,
         null,
         null
        ],
        [
         "3187",
         "spam",
         "This is the 2nd time we have tried 2 contact u. U have won the 750 Pound prize. 2 claim is easy, call 08712101358 NOW! Only 10p per min. BT-national-rate",
         null,
         null,
         null
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5020</th>\n",
       "      <td>ham</td>\n",
       "      <td>:-( sad puppy noise</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4577</th>\n",
       "      <td>spam</td>\n",
       "      <td>Urgent! call 09066350750 from your landline. Y...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1360</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yo dude guess who just got arrested the other day</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4440</th>\n",
       "      <td>ham</td>\n",
       "      <td>I'm going 2 orchard now laready me reaching so...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3187</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        v1                                                 v2 Unnamed: 2  \\\n",
       "5020   ham                                :-( sad puppy noise        NaN   \n",
       "4577  spam  Urgent! call 09066350750 from your landline. Y...        NaN   \n",
       "1360   ham  Yo dude guess who just got arrested the other day        NaN   \n",
       "4440   ham  I'm going 2 orchard now laready me reaching so...        NaN   \n",
       "3187  spam  This is the 2nd time we have tried 2 contact u...        NaN   \n",
       "\n",
       "     Unnamed: 3 Unnamed: 4  \n",
       "5020        NaN        NaN  \n",
       "4577        NaN        NaN  \n",
       "1360        NaN        NaN  \n",
       "4440        NaN        NaN  \n",
       "3187        NaN        NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89ee545",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bf98bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   v1          5572 non-null   object\n",
      " 1   v2          5572 non-null   object\n",
      " 2   Unnamed: 2  50 non-null     object\n",
      " 3   Unnamed: 3  12 non-null     object\n",
      " 4   Unnamed: 4  6 non-null      object\n",
      "dtypes: object(5)\n",
      "memory usage: 217.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ff552e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop last 3 cols\n",
    "df.drop(columns=['Unnamed: 2','Unnamed: 3','Unnamed: 4'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2bf6f1",
   "metadata": {},
   "source": [
    "### Renaming the Features v1 as target and v2 as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6240e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming the cols\n",
    "df.rename(columns={'v1':'target','v2':'text'},inplace=True)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0206b8",
   "metadata": {},
   "source": [
    "## Encoding the Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136b84be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2cf6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = encoder.fit_transform(df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9f4176",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoder.classes_) # 0->ham and 1-> spam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a23f12",
   "metadata": {},
   "source": [
    "### Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b167ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f10eaf",
   "metadata": {},
   "source": [
    "### Check for duplicate values and remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04642f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e480b12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(keep='first')\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b12716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ab0434",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca77ff3",
   "metadata": {},
   "source": [
    "**Using Countplot check the distribution of the target.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8b2ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='target', data=df, hue='target')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d02ab8",
   "metadata": {},
   "source": [
    "### Now we have to do some Natural Language Processing on the the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cad58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d9e7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da11976",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aab7b8",
   "metadata": {},
   "source": [
    "It downloads the Punkt model, which is a pre-trained data file that helps the NLTK library split text into lists of words (tokenization) and sentences.\n",
    "\n",
    "Without it, NLTK cannot intelligently break paragraphs into sentences or words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51a8bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_characters'] = df['text'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a4f1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3617c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num of words\n",
    "df['num_words'] = df['text'].apply(lambda x:len(nltk.word_tokenize(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfee60e8",
   "metadata": {},
   "source": [
    "It creates a new column called **`num_words`** that contains the **total count of words** for each message in the `text` column.\n",
    "\n",
    "**Example:**\n",
    "* **Input (`x`):** `\"Hi, how are you?\"`\n",
    "* **Tokenized:** `['Hi', ',', 'how', 'are', 'you', '?']`\n",
    "* **Result (`num_words`):** `6`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a3b777",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_sentences'] = df['text'].apply(lambda x:len(nltk.sent_tokenize(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caddbd8",
   "metadata": {},
   "source": [
    "It creates a new column called **`num_sentences`** that contains the **total count of sentences** for each message.\n",
    "\n",
    "**Example:**\n",
    "* **Input (`x`):** `\"I am fine. How are you?\"`\n",
    "* **Tokenized:** `['I am fine.', 'How are you?']`\n",
    "* **Result (`num_sentences`):** `2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26728e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['num_characters','num_words','num_sentences']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd803b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ham\n",
    "df[df['target'] == 0][['num_characters','num_words','num_sentences']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47cd636",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spam\n",
    "df[df['target'] == 1][['num_characters','num_words','num_sentences']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b037bd",
   "metadata": {},
   "source": [
    "**Make a histplot for different targets based on number of characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973968dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.histplot(df[df['target'] == 0]['num_characters']) #ham\n",
    "sns.histplot(df[df['target'] == 1]['num_characters'],color='red') #spam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa4837f",
   "metadata": {},
   "source": [
    "**Make a Pairplot but it should show distinctly the targets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eae0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.histplot(df[df['target'] == 0]['num_words'])\n",
    "sns.histplot(df[df['target'] == 1]['num_words'],color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cd59dc",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "- Lower case\n",
    "- Tokenization\n",
    "- Removing special characters\n",
    "- Removing stop words and punctuation\n",
    "- Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47641ed4",
   "metadata": {},
   "source": [
    "## Text Preprocessing Steps\n",
    "\n",
    "| Step | Definition | Importance (Why?) |\n",
    "| :--- | :--- | :--- |\n",
    "| **Lower Casing** | Converts all text to **lowercase** (e.g., \"The\" $\\rightarrow$ \"the\"). | Ensures the model treats variations like \"Apple\" and \"apple\" as the **same word**, reducing vocabulary size. |\n",
    "| **Tokenization** | Breaks the text into its smallest meaningful units (words, numbers, punctuation). | The **mandatory first step**; it converts a raw string into a list of items for counting and processing. |\n",
    "| **Removing Special Characters** | Eliminates non-alphanumeric symbols (e.g., `@`, `#`, `&`). | Reduces **noise** and keeps the model focused only on characters relevant to language. |\n",
    "| **Removing Stop Words & Punctuation** | Removes common, high-frequency words (e.g., \"a,\" \"the,\" \"is\") and standard punctuation. | Drastically reduces the number of features and forces the model to learn from **meaningful, distinguishing words** (like 'spam' keywords). |\n",
    "| **Stemming** | Reduces a word to its base or root form (e.g., \"running,\" \"runs\" $\\rightarrow$ \"run\"). | Reduces the feature space by treating all grammatical variations of a word as a single feature, helping the model **generalize** better. |\n",
    "\n",
    "***\n",
    "\n",
    "### Overall Importance: Model Effectiveness\n",
    "\n",
    "These steps are vital because they convert raw, messy human language into a clean, consistent, and numerical format that machine learning algorithms can process effectively. They ensure the model is **efficient**, **consistent**, and focuses only on the most **distinguishing information**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3591de72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def transform_text(text):\n",
    "    text = text.lower()\n",
    "    text = nltk.word_tokenize(text)\n",
    "    \n",
    "    y = []\n",
    "    for i in text:\n",
    "        if i.isalnum():\n",
    "            y.append(i)\n",
    "    \n",
    "    text = y[:]\n",
    "    y.clear()\n",
    "    \n",
    "    for i in text:\n",
    "        if i not in stopwords.words('english') and i not in string.punctuation:\n",
    "            y.append(i)\n",
    "            \n",
    "    text = y[:]\n",
    "    y.clear()\n",
    "    \n",
    "    for i in text:\n",
    "        y.append(ps.stem(i))\n",
    "    \n",
    "            \n",
    "    return \" \".join(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c52c97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_text(\"I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69c2853",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['transformed_text'] = df['text'].apply(transform_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d669b27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3969a795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "wc = WordCloud(width=500,height=500,min_font_size=10,background_color='white')\n",
    "\n",
    "spam_wc = wc.generate(df[df['target'] == 1]['transformed_text'].str.cat(sep=\" \"))\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.imshow(spam_wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff416d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_wc = wc.generate(df[df['target'] == 0]['transformed_text'].str.cat(sep=\" \"))\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.imshow(ham_wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2235d164",
   "metadata": {},
   "source": [
    "# Model Building\n",
    "\n",
    "##  What is Vectorization?\n",
    "Vectorization is the process of converting text data into numerical data that machine learning models can understand.\n",
    "Vectorization (specifically **Text Vectorization**) is the essential step of turning unstructured text (like a word, a sentence, or a document) into a sequence of numbers, known as a **vector**.\n",
    "\n",
    "For example, a sentence like \"I love cats\" might be converted into the vector: `[0, 1, 0, 1, 2]` where each number corresponds to the presence or frequency of a specific word in the entire vocabulary.\n",
    "\n",
    "### Types:\n",
    "\n",
    "1.  **CountVectorizer (`cv`):** Converts text into a vector by simply counting the frequency of each word in a document.\n",
    "    * *Example:* If the word \"free\" appears 5 times, its corresponding number in the vector is 5.\n",
    "2.  **TfidfVectorizer (`tfidf`):** Converts text into a vector based on **Term Frequency-Inverse Document Frequency (TF-IDF)**. This is a more sophisticated method that weighs word count by how rare or important the word is across *all* documents.\n",
    "    * *Goal:* Give high scores to words that appear often in a *specific* document but rarely in the entire dataset (e.g., \"lottery\" in a spam message).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2df8b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "cv = CountVectorizer()\n",
    "tfidf = TfidfVectorizer(max_features=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1ffe0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf.fit_transform(df['transformed_text']).toarray()\n",
    "y = df['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d99d914",
   "metadata": {},
   "source": [
    "**Train Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3221a5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53889af6",
   "metadata": {},
   "source": [
    "# Model Building Prediction and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0c35a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,precision_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc22bf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "mnb = MultinomialNB()\n",
    "bnb = BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda615ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.fit(X_train,y_train)\n",
    "y_pred1 = gnb.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred1))\n",
    "print(confusion_matrix(y_test,y_pred1))\n",
    "print(precision_score(y_test,y_pred1))\n",
    "print(classification_report(y_test,y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089f8847",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.fit(X_train,y_train)\n",
    "y_pred2 = mnb.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred2))\n",
    "print(confusion_matrix(y_test,y_pred2))\n",
    "print(precision_score(y_test,y_pred2))\n",
    "print(classification_report(y_test,y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79c26f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb.fit(X_train,y_train)\n",
    "y_pred3 = bnb.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred3))\n",
    "print(confusion_matrix(y_test,y_pred3))\n",
    "print(precision_score(y_test,y_pred3))\n",
    "print(classification_report(y_test,y_pred1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
